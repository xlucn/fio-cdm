#!/usr/bin/env python
import os
import sys
import glob
import json
import argparse
import logging
import configparser
import subprocess

logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))

# TODO: figure out the filenames, thread numbers, etc
class Job:
    def __init__(self, size, numjobs, directory):
        self.jobs = []
        self.bandwidth = {}
        self.config = configparser.ConfigParser(allow_no_value=True)
        self.config.read_dict({
            'global': {
                'ioengine': 'libaio',
                # TODO: time vary with number of jobs
                'runtime': '120',
                'size': size,
                # TODO: numjobs seems to be the thread number, change it
                'numjobs': numjobs,
                # TODO
                #  'filename': filename,
                'directory': directory,
                'fdatasync': '1',
                'direct': '1',
                'refill_buffers': None,
                'norandommap': None,
                'randrepeat': '0',
                'allrandrepeat': '0',
                'group_reporting': None
            }
        })

    def _jobname_templ(self, job_dict):
        jobname = "Seq-" if job_dict["rw"] == 'seq' else "Rand-"
        jobname += "{rw}-"
        jobname += "{}".format(job_dict["bs"].upper())
        jobname += "-Q{}".format(job_dict["q"]) if job_dict["q"] > 1 else ""
        jobname += "-T{}".format(job_dict["t"]) if job_dict["t"] > 1 else ""
        return jobname

    def _displayname(self, job_dict):
        displayname = "SEQ" if job_dict["rw"] == 'seq' else "RND"
        displayname += "{} ".format(job_dict["bs"].upper())
        displayname += "Q{:2}".format(str(job_dict["q"]))
        displayname += "T{:2}".format(str(job_dict["t"]))
        return displayname

    def create_jobfile(self, jobfile):
        if jobfile == '-':
            self.config.write(sys.stdout, space_around_delimiters=False)
        else:
            with open(jobfile, 'w') as f:
                self.config.write(f, space_around_delimiters=False)

    def create_job(self, rw_type, queue_size, thread_num):
        # fix the block size to 1M (sequential) or 4K (random)
        blocksize = '1m' if rw_type == 'seq' else '4k'

        job = {
            "rw": rw_type,
            "bs": blocksize,
            "q": queue_size,
            "t": thread_num
        }
        self.jobs.append(job)
        jobname_templ = self._jobname_templ(job)

        for name, rw in [("Read","read"), ("Write", "write"), ("Mix", "rw")]:
            self.config.read_dict({
                jobname_templ.format(rw=name): {
                    'iodepth': queue_size,
                    'bs': blocksize,
                    'rw': rw if rw_type == 'seq' else 'rand' + rw,
                    'rwmixread': 50,
                    'stonewall': None
                }
            })

    def run(self):
        jobfile = 'jobfile'
        if args.dump_jobfile:
            self.create_jobfile(args.dump_jobfile)
            exit()
        else:
            self.create_jobfile(jobfile)

        if not self._check_disk_space():
            exit(1)

        try:
            cmd = ['fio', '--output-format', 'json', jobfile]
            output = subprocess.check_output(cmd)
        except KeyboardInterrupt:
            print('interrupted, cleaning up before exit...')
            exit(1)
        finally:
            # TODO: clean up
            self._clean_testfiles()
            os.remove(jobfile)

        logging.debug(output.decode('utf-8'))
        info = json.loads(output)

        # Unit of I/O speed, use MB/s(10^6) instead of MiB/s(2^30).
        unit = 10**6

        # TODO: don't feel right about this way of storing data
        for job in info['jobs']:
            rw = job['job options']['rw']
            if rw == 'read' or rw == 'randread':
                self.bandwidth[job['jobname']] = job['read']['bw_bytes'] / unit
            if rw == 'write' or rw == 'randwrite':
                self.bandwidth[job['jobname']] = job['write']['bw_bytes'] / unit
            if rw == 'rw' or rw == 'randrw':
                self.bandwidth[job['jobname']] = job['write']['bw_bytes'] / unit

    def print_result(self):
        if args.mix:
            print("|Name        | Read  | Write |  Mix  |\n"
                  "|------------|-------|-------|-------|")
            template_row = "|{jobname}|{read:7.2f}|{write:7.2f}|{mix:7.2f}|"
        else:
            print("|Name        | Read  | Write |\n"
                  "|------------|-------|-------|")
            template_row = "|{jobname}|{read:7.2f}|{write:7.2f}|"

        for job in self.jobs:
            jobname = self._jobname_templ(job)
            print(template_row.format(
                jobname=self._displayname(job),
                read=self.bandwidth[jobname.format(rw="Read")],
                write=self.bandwidth[jobname.format(rw="Write")],
                mix=self.bandwidth[jobname.format(rw="Mix")] if args.mix else 0
            ))

    def _clean_testfiles(self):
        for job in self.jobs:
            jobname = self._jobname_templ(job)
            for rw in ["Read", "Write", "Mix"]:
                for f in glob.glob(jobname.format(rw=rw) + "*"):
                    os.remove(f)

    def _check_disk_space(self):
        # TODO: not sure
        statvfs = os.statvfs(args.target)
        avail = statvfs.f_frsize * statvfs.f_bfree
        needed = 0
        for job in self.jobs:
            bs = 2**20 if job["bs"] == "1m" else 4096
            needed += bs * args.number
        logging.debug("Target: %s\navailable: %s\nneeded: %s",
                      args.target, avail, needed)
        if avail > needed:
            return True
        else:
            logging.warning("Not enough space available in %s:", args.target)
            logging.warning("Needed: %s. Available: %s", needed, avail)
            return False


def get_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument('target', help='The path of the directory to test')
    parser.add_argument('-n', metavar='number',
                        type=int,
                        default=1,
                        help='Number of tests, default to 1')
    parser.add_argument('-s', metavar='size',
                        default='1G',
                        help='The size of file I/O, same as the fio parameter'
                             ', default to 1G')
    parser.add_argument('-x', metavar='mix',
                        type=int,
                        nargs="?",
                        const=70,
                        default=0,
                        help='Add mixed rw test, default is not add it'
                             '. <mix> is read percentage (integer), default to 70'
                             '. Note: don\'t add argument <target> after -x'
                             ', it will be regarded as <mix> but an error'
                             ' will pop up since -x need a integer.')
    parser.add_argument('-f', metavar='dump-jobfile',
                        help='Save jobfile and quit without running fio'
                             '. Use \'-\' to print to stdout'
                             '. Tip: show the equivalent command: '
                             'fio-cdm -f - | fio --showcmd -')
    return parser


if __name__ == '__main__':
    # TODO: realtime visual feedback, with fio --debug
    # TODO: time upper limit do this with 'runtime'
    # TODO: show iops or latency
    parser = get_parser()
    args = parser.parse_args()
    logging.debug(args)

    fio_job = Job(args.size, args.number, args.target)
    fio_job.create_job('seq', 1, 1)
    #  fio_job.create_job('rand', 1, 1)
    #  fio_job.create_job('rand', 16, 1)
    #  fio_job.create_job('rand', 32, 8)
    fio_job.run()
    fio_job.print_result()
